---
title: "Timeline of Keynotes"
theme: "damon-light"
---

<style>
  /* The actual timeline (the vertical ruler) */
  .main-timeline {
    position: relative;
  }

  /* The actual timeline (the vertical ruler) */
  .main-timeline::after {
    content: '';
    position: absolute;
    width: 6px;
    background-color: #939597;
    top: 0;
    bottom: 0;
    left: 50%;
    margin-left: -3px;
  }

  /* Container around content */
  .timeline {
    position: relative;
    background-color: inherit;
    width: 50%;
  }

  /* The circles on the timeline */
  .timeline::after {
    content: '';
    position: absolute;
    width: 25px;
    height: 25px;
    right: -13px;
    background-color: #939597;
    border: 5px solid #f6d39d;
    top: 15px;
    border-radius: 50%;
    z-index: 1;
  }

  /* Place the container to the left */
  .left {
    padding: 0px 40px 20px 0px;
    left: 0;
  }

  /* Place the container to the right */
  .right {
    padding: 0px 0px 20px 40px;
    left: 50%;
  }

  /* Add arrows to the left container (pointing right) */
  .left::before {
    content: " ";
    position: absolute;
    top: 18px;
    z-index: 1;
    right: 30px;
    border: medium solid white;
    border-width: 10px 0 10px 10px;
    border-color: transparent transparent transparent white;
  }

  /* Add arrows to the right container (pointing left) */
  .right::before {
    content: " ";
    position: absolute;
    top: 18px;
    z-index: 1;
    left: 30px;
    border: medium solid white;
    border-width: 10px 10px 10px 0;
    border-color: transparent white transparent transparent;
  }

  /* Fix the circle for containers on the right side */
  .right::after {
    left: -12px;
  }

  /* Media queries - Responsive timeline on screens less than 600px wide */
  @media screen and (max-width: 600px) {

    /* Place the timelime to the left */
    .main-timeline::after {
      left: 31px;
    }

    /* Full-width containers */
    .timeline {
      width: 100%;
      padding-left: 70px;
      padding-right: 25px;
    }

    /* Make sure that all arrows are pointing leftwards */
    .timeline::before {
      left: 60px;
      border: medium solid white;
      border-width: 10px 10px 10px 0;
      border-color: transparent white transparent transparent;
    }

    /* Make sure all circles are at the same spot */
    .left::after,
    .right::after {
      left: 18px;
    }

    .left::before {
      right: auto;
    }

    /* Make all right containers behave like the left ones */
    .right {
      left: 0%;
    }
  }
</style>

<h1 id="damon-keynotes-timeline">Damon Keynotes Timeline</h1>
<div class="container py-5">
  <div class="main-timeline">
    <div class="timeline left">
      <div class="card">
        <div class="card-body p-4">
          <h3>2005</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">Data Management Challenges on New Computer Architectures
            </summary>
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Doug Carmean (Intel)</summary>
          </details>
        </div>
      </div>
    </div>
    <div class="timeline right">
      <div class="card">
        <div class="card-body p-4">
          <h3>2006</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">Information Management and System/Storage Technology —
              Evolution or Revolution?</summary>
            The role and design of systems responsible for the management of information in the enterprise is changing.
            The kind of information that is being managed is changing as is the way the information is analyzed and made
            available to users in the enterprise.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Berni Schiefer (IBM Toronto)</summary>
            Berni Schiefer is a DB2 Distinguished Engineer at IBM. He has responsibility for DB2 performance
            benchmarking and solutions development, including the BCU. He joined the IBM Toronto Lab in 1985 and has
            worked on SQL/DS and the Starburst experimental relational database at the IBM Almaden Research Lab, prior
            to working on DB2. His current focus is on introducing advanced technology into DB2 with particular emphasis
            on processors, performance, XML, Linux, Virtualization and Autonomics.
          </details>
        </div>
      </div>
    </div>
    <div class="timeline left">
      <div class="card">
        <div class="card-body p-4">
          <h3>2007</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">How do DBMS take advantage of future computer systems?
            </summary>
            Historically, CMOS scaling provides certain level of performance enhancement automatically. However, that
            “free” performance enhancement from device scaling will come to an end while CMOS scaling will continue for
            several more generations. Multi-core has been one architectural feature to improve chip level performance.
            Partially because of the power dissipation limit, each core of a multi-core chip becomes simpler/smaller and
            offers weaker single thread performance. In this talk, we will explain how to avoid potential performance
            bottlenecks when running typical DBMS software on a massive multi-core chip. For a high-end transaction
            system, the main memory cost is easily several times of CPU cost; the storage cost is even higher than the
            main memory cost. We will examine how potential future memory technologies (such as phase-change memory) may
            impact computer system architecture. A new class of high volume transaction systems is emerging. Each
            transaction is relatively simple. However, the potential revenue for each transaction may be very low. Thus,
            the transaction systems designed for banking-like applications may not be suitable for this new type of
            applications. We will describe the problem and encourage researchers and practitioners to come up with
            cost-effective solutions.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Honesty Young (IBM)</summary>
            Dr. Honesty Young earned his Ph.D. in Computer Science from University of Wisconsin-Madison. Currently he is
            the Deputy Director and the CTO of IBM China Research Lab. He helped build the first parallel database
            prototype inside IBM. He led an effort that achieved leadership TPC database benchmark results. He has
            initiated and managed projects in storage appliances and controllers. He spent a year at IBM Research
            Division Headquarters as a technical staff. Dr. Young has published more than 40 journal and conference
            papers, including one best paper and one invited paper. He was the Industrial Program Chair of the Parallel
            and Distributed Information Systems (PDIS), taught two tutorials at key conferences, and served on the
            program committees of eight conferences. He is an IBM Master Inventor.
          </details>
        </div>
      </div>
    </div>
    <div class="timeline right">
      <div class="card">
        <div class="card-body p-4">
          <h3>2008</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">Amorphous Data Parallelism</summary>
            Client-side applications running on multicore processors are likely to be irregular programs that deal with
            complex, pointer-based data structures such as graphs and trees. In her 2007 Turing award lecture, Fran
            Allen raised an important question about such programs: do irregular programs have data parallelism, and if
            so, how do we exploit it on multicore processors? In this talk, we argue using concrete examples that
            irregular programs have an amorphous data-parallelism that arises from the use of iterative algorithms that
            manipulate worklists of various sorts. We then describe the approach taken in the Galois project to exploit
            this parallelism. There are three main aspects to the Galois system: (1) a small number of syntactic
            constructs for packaging amorphous data-parallelism as iterations over ordered and unordered sets, (2)
            assertions about methods in class libraries, and (3) a runtime system for managing the exploitation of
            amorphous data-parallelism. We present experimental results that demonstrate that the Galois approach is
            practical, and discuss ongoing work on this system.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Keshav Pingali (University of Texas, Austin)</summary>
            Keshav Pingali is the W.A."Tex" Moncrief Chair of Computing in the Computer Sciences department at the
            University of Texas, Austin. He received the B.Tech. degree in Electrical Engineering from IIT, Kanpur,
            India in 1978, the S.M. and E.E. degrees from MIT in 1983, and the Sc.D. degree from MIT in 1986. He was on
            the faculty of the Department of Computer Science at Cornell University from 1986 to 2006, where he held the
            India Chair of Computer Science. Pingali's research has focused on programming languages and compiler
            technology for program understanding, restructuring, and optimization. His group is known for its
            contributions to memory-hierarchy optimization; some of these have been patented. Algorithms and tools
            developed by his projects are used in many commercial products such as Intel's IA-64 compiler, SGI's MIPSPro
            compiler, and HP's PA-RISC compiler. In his current research, he is investigating optimistic parallelization
            techniques for multicore processors, and language-based fault tolerance. Among other awards, Pingali has won
            the President's Gold Medal at I.I.T. Kanpur (1978), IBM Faculty Development Award (1986-88), NSF
            Presidential Young Investigator Award (1989-94), Ip-Lee Teaching Award of the College of Engineering at
            Cornell (1997), and the Russell teaching award of the College of Arts and Sciences at Cornell (1998). In
            2000, he was a visiting professor at I.I.T., Kanpur where he held the Rama Rao Chaired Professorship. Since
            2007, he has been the co-Editor-in-chief of the ACM Transactions on Programming Languages and Systems
            (TOPLAS).
          </details>
        </div>
      </div>
    </div>
    <div class="timeline left">
      <div class="card">
        <div class="card-body p-4">
          <h3>2009</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">Sweet Sixteen: How well is Transactional Memory Aging?
            </summary>
            The term ``Transactional Memory'' was coined back in 1993, but even today, there is a vigorous debate about
            its merits. This debate sometimes generates more heat than light: terms are not always well-defined and
            criteria for making judgments are not always clear. In this talk, I will try to impose some order on the
            conversation. TM itself can encompass hardware, software, speculative lock elision, and other mechanisms.
            The benefits sought encompass simpler implementations of highly-concurrent data structures, better software
            engineering for concurrent platforms, enhanced performance, and reduced power consumption. We will look at
            various terms in this cross-product and evaluate how we are doing. So far.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Maurice Herlihy (Brown University)</summary>
            Maurice Herlihy has an A.B. in Mathematics from Harvard University, and a Ph.D. in Computer Science from
            M.I.T. He has served on the faculty of Carnegie Mellon University and the staff of DEC Cambridge Research
            Lab. He is the recipient of the 2003 Dijkstra Prize in Distributed Computing, the 2004 Gödel Prize in
            theoretical computer science, the 2008 ISCA influential paper award, the 2012 Edsger W. Dijkstra Prize, and
            the 2013 Wallace McDowell award. He received a 2012 Fulbright Distinguished Chair in the Natural Sciences
            and Engineering Lecturing Fellowship, and he is fellow of the ACM, a fellow of the National Academy of
            Inventors, the National Academy of Engineering, and the National Academy of Arts and Sciences.
          </details>
        </div>
      </div>
    </div>
    <div class="timeline right">
      <div class="card">
        <div class="card-body p-4">
          <h3>2010</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">Trends in Storage Technologies</summary>
            This presentation highlights some of the leading-edge topics in storage technology research today. The main
            focus will be online storage and specifically solid-state-based storage-class memory (SCM), which has the
            potential of revolutionizing architectures for data storage. In this context, we will review the application
            of Flash to enterprise storage and discuss high-potential follow-on technologies in this space and their
            implications for the memory/storage hierarchy. Moreover, we will also look at the other extreme, namely
            storage technologies for archival data, and discuss how the explosive growth of such data and the subsequent
            ultra-high capacity requirements affect the incumbent technologies such as magnetic tape and optical
            archives. Finally, we'll touch upon the impact of these disruptive technologies on the tiered storage.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Dr. Evangelos Eleftheriou (IBM Fellow, IBM Zurich Lab)
            </summary>
            Dr. Eleftheriou received a B.S. degree in electrical engineering from the University of Patras, Patras,
            Greece, in 1979, and the M.Eng. and Ph.D. degrees in electrical engineering from Carleton University,
            Ottawa, Canada, in 1981 and 1985, respectively. In 1986, he joined IBM Research. Zurich, where he currently
            manages the Storage Technologies Department, which focuses on phase-change memories, scanning-probe
            techniques and metrology, solid-state drive technology and systems as well as tape drive technology. In
            January 2002 Dr. Eleftheriou was elected Fellow of the IEEE. He was co-recipient of the 2003 IEEE Leonard G.
            Abraham Prize Paper Award and co-recipient of the Eduard Rhein Technology Award in 2005. The same year, he
            became an IBM Fellow and was inducted into the IBM Academy of Technology. In 2009 he was co-recipient of the
            IEEE Transactions on Control Systems Technology Outstanding Paper Award and the IEEE Control Systems
            Technology Award.
          </details>
        </div>
      </div>
    </div>
    <div class="timeline left">
      <div class="card">
        <div class="card-body p-4">
          <h3>2011</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">The coming revolution in data-centric data centers
            </summary>
            We are entering an exciting era for systems design. Digital information is increasing at exponential rates
            and new applications are being developed to extract fresh insights from all this data. At the same time, we
            are seeing interesting inflection points in technology - faster, more complex, processors are being replaced
            by simpler, power-efficient multicores; traditional memory and storage technologies are being challenged by
            new non-volatile memories like phase-change RAM and Memristors; optics is replacing electrical
            communications. Consequently, traditional approaches to design "better, faster, cheaper" systems will need
            to be (and are being) rethought, both at the hardware and software levels. In this talk, I will discuss
            these recent trends, their implications for future hardware re-designs, and the immense opportunities ahead
            for new solutions that cross-cut the technology, architecture, and software layers.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Parthasarathy Ranganathan (HP Labs)</summary>
            Partha Ranganathan is a distinguished technologist at Hewlett Packard Labs where he currently leads a
            research program on future data-centric data centers. His research interests are in energy efficiency and
            systems architecture and modeling. He has worked extensively in these areas including key contributions
            around energy-aware user interfaces, heterogeneous multi-core processors, power capping and federated
            enterprise power management, energy modeling and benchmarking, disaggregated blade server architectures, and
            most recently, storage hierarchy and systems redesign for non-volatile memory. Dr. Ranganathan's work has
            led to several commercial products and has been featured in the New York Times, Wall Street Journal,
            Slashdot, and several other venues. He was named one of the world's top young innovators by MIT Technology
            Review, and has received Rice University's Outstanding Young Engineering Alumni award. Dr. Ranganathan
            received his B.Tech degree from the Indian Institute of Technology, Madras and his M.S. and Ph.D. from Rice
            University, Houston.
          </details>
        </div>
      </div>
    </div>
    <div class="timeline right">
      <div class="card">
        <div class="card-body p-4">
          <h3>2012</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">Redrawing the Boundary Between Software and Storage for
              Fast Non-Volatile Memories</summary>
            The emerging technology of fast non-volatile memories (NVMs) such as Phase Change Memory (PCM) and
            Spin-Torque Transfer Magnetic RAM (STT-RAM) promises to fill the gap between main memory and block-oriented
            storage, which has existed for over three decades. However, the performance characteristics and access
            mechanisms of NVMs differ significantly from those of traditional DRAM and storage devices, necessitating
            the redesign of software and storage systems to fully exploit their potential. In this talk, I will present
            an overview of our recent work on rethinking the boundary between software and storage systems to take
            advantage of fast NVMs. I will discuss new storage architectures, algorithms, and data structures that
            enable efficient and scalable use of NVMs in a variety of computing environments.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Steven Swanson (UCSD)</summary>
            Steven Swanson is an Associate Professor in the Department of Computer Science and Engineering at the
            University of California, San Diego and the director of the Non-Volatile Systems Laboratory. His research
            interests include the systems, architecture, security, and reliability issues surrounding non-volatile,
            solid-state memories. He also co-leads the Science and Engineering of Non-Volatile Systems (SENS) center, a
            major research collaboration between industry and academia that is exploring the fundamental physics,
            materials, and designs underlying tomorrow's non-volatile memories. He is also a founder of the Non-Volatile
            Memories Workshop, the premier venue for research in these areas. He received his PhD from the University of
            Washington in 2006 and was a founder of the Center for Research in Emerging Technologies, a joint research
            center between the University of Washington and Microsoft Research. He is the recipient of a Sloan
            Fellowship, a Microsoft Research New Faculty Fellowship, an NSF CAREER Award, and a selection as one of
            Scientific American's 50 world-changing researchers.
          </details>
        </div>
      </div>
    </div>
    <div class="timeline left">
      <div class="card">
        <div class="card-body p-4">
          <h3>2013</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">Netezza Performance Architecture</summary>
            In this talk I will explore hardware and software co-design as a strategy for developing novel, high value data management products on new hardware through the lens of the early history of Netezza Corporation. In a little over ten years, Netezza went from a standing start to a successful public offering and then a $1.7B acquisition by IBM. By combining field programmable gate arrays, loosely coupled multi-processing based on low power CPUs and data structures specialized for analytical query processing with low demand for administration and database tuning on the part of the user, Netezza successfully disrupted the market for data warehousing solutions. The key to this success was a novel combination of hardware and software that yielded market leading price-performance and market leading ease-of-use. Both of these characteristics were directly tied to key technical decisions that re-imagined both database management software and hardware, a process referred to here as 'deep codesign'.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Daniel J. "Dan" Feldman (Netezza)</summary>
            Daniel J. "Dan" Feldman is a Senior Fellow and the former Chief Scientist at Netezza, where he worked on the
            design and optimization of data warehouse appliance systems. He has over thirty years of experience in
            computer systems architecture and performance analysis, and has worked on a wide range of systems including
            large-scale multiprocessors, network processors, and high-performance network systems. Before joining
            Netezza, he was a member of the research staff at the IBM Thomas J. Watson Research Center, where he worked
            on high-performance networking, network processors, and related topics. He has published over sixty
            technical papers and holds over twenty patents in these areas. He is a Senior Member of the IEEE, and
            received his Ph.D. in Electrical Engineering from Stanford University.
          </details>
        </div>
      </div>
    </div>
    <div class="timeline right">
      <div class="card">
        <div class="card-body p-4">
          <h3>2014</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">The Picosecond is Dead; Long Live the Picojoule</summary>
            For decades, CMOS technology provided exponential improvements in transistor density and energy consumption.
            However, power constraints have become the primary challenge in scaling transistor performance, leading to
            the end of Dennard scaling. As a result, computer architects are increasingly focusing on energy efficiency
            rather than raw performance as the primary design goal. This talk will explore the implications of this
            shift for computer architecture, focusing on energy-efficient architectures, near-threshold computing, and
            the co-design of hardware and software for energy efficiency.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Christos Kozyrakis (Stanford University)</summary>
            Christos Kozyrakis is an Associate Professor of Electrical Engineering and Computer Science at Stanford
            University. His research interests are in computer architecture, systems, and energy-efficient computing. He
            has published more than 100 papers in major conferences and journals and is the recipient of an NSF CAREER
            Award, an IBM Faculty Award, Google Faculty Awards, the 2010 ACM SIGARCH Maurice Wilkes Award, and the 2018
            IEEE Computer Society Edward J. McCluskey Technical Achievement Award. He has also received several
            best-paper awards and nominations at ASPLOS, ISCA, MICRO, and HPCA. He is an ACM Fellow and an IEEE Fellow.
          </details>
        </div>
      </div>
    </div>

    <!-- 2015 -->
    <div class="timeline left">
      <div class="card">
        <div class="card-body p-4">
          <h3>2015</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">Rethinking Memory System Design for Data-Intensive
              Computing</summary>
            The memory system is a fundamental performance and energy bottleneck in almost all computing systems. Recent
            system design, application, and technology trends that require more capacity, bandwidth, efficiency, and
            predictability out of the memory system make it an even more important system bottleneck. At the same time,
            DRAM and flash technologies are experiencing difficult technology scaling challenges that make the
            maintenance and enhancement of their capacity, energy-efficiency, and reliability significantly more costly
            with conventional techniques.
            In this talk, we examine some promising research and design directions to overcome challenges posed by
            memory scaling. Specifically, we discuss three key solution directions: 1) enabling new memory
            architectures, functions, interfaces, and better integration of the memory and the rest of the system, 2)
            designing a memory system that intelligently employs multiple memory technologies and coordinates memory and
            storage management using non-volatile memory technologies, 3) providing predictable performance and QoS to
            applications sharing the memory/storage system. If time permits, we might also briefly touch upon our
            ongoing related work in combating scaling challenges of NAND flash memory.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Onur Mutlu (Carnegie Mellon University)</summary>
            Onur Mutlu is the Strecker Early Career Professor at Carnegie Mellon University. His broader research
            interests are in computer architecture and systems, especially in the interactions between languages, system
            software, compilers, and microarchitecture, with a major current focus on memory systems. He obtained his
            PhD and MS in ECE from the University of Texas at Austin and BS degrees in Computer Engineering and
            Psychology from the University of Michigan, Ann Arbor. Prior to Carnegie Mellon, he worked at Microsoft
            Research, Intel Corporation, and Advanced Micro Devices. He was a recipient of the IEEE Computer Society
            Young Computer Architect Award, Intel Early Career Faculty Award, faculty partnership awards from various
            companies, including Facebook, Google, HP, Intel, IBM, Microsoft and Samsung, a number of best paper
            recognitions at various computer systems venues, and a number of "computer architecture top pick" paper
            selections by the IEEE Micro magazine. For more information, please see his webpage.
          </details>
        </div>
      </div>
    </div>

    <!-- 2016 -->
    <div class="timeline right">
      <div class="card">
        <div class="card-body p-4">
          <h3>2016</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">Looking Beyond Exaflops and Zettabytes</summary>
            We are seeing an unprecedented convergence of massive compute with massive data. This confluence has the
            potential to significantly impact how we do computing and what computing can do for us. In this talk I will
            discuss some of the application-level opportunities and system-level data management challenges at the
            intersection of traditional high-performance computing and emerging data-intensive computing.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Pradeep Dubey (Intel Labs)</summary>
            Pradeep Dubey is an Intel Fellow and Director of Parallel Computing Lab (PCL), part of Intel Labs. His
            research focus is computer architectures to efficiently handle new compute- and data-intensive application
            paradigms for the future computing environment. Dubey previously worked at IBM's T.J. Watson Research
            Center, and Broadcom Corporation. He has made contributions to the design, architecture, and
            application-performance of various microprocessors, including IBM® Power PC*, Intel® i386TM, i486TM,
            Pentium® Xeon®, and the Xeon Phi™ line of processors. He holds over 36 patents, has published over 100
            technical papers, won the Intel Achievement Award in 2012 for Breakthrough Parallel Computing Research, and
            was honored with Outstanding Electrical and Computer Engineer Award from Purdue University in 2014.
          </details>
        </div>
      </div>
    </div>

    <!-- 2017 -->
    <div class="timeline left">
      <div class="card">
        <div class="card-body p-4">
          <h3>2017</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">HPC & AI</summary>
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Dr. Eng Lim Goh (HPE)</summary>
            Dr. Eng Lim Goh joined SGI in 1989, becoming a chief engineer in 1998 and then chief technology officer in
            2000. After acquisition, HPE appointed him vice president and SGI chief technology officer. He oversees
            technical computing programs with the goal to develop the next generation computer architecture for the new
            many-core era. His current research interest is in the progression from data intensive computing to
            analytics, machine learning, artificial specific to general intelligence and autonomous systems. He
            continues his studies in human perception for user interfaces and virtual and augmented reality.
          </details>
          <hr>
          <details>
            <summary style="list-style: none; color: #334d7d;">The Latest Advances in GPU Architectures and New
              Programming Model Features</summary>
            GPU architectures are approaching a terabyte per second memory bandwidth that, coupled with high-throughput
            computational cores, creates an ideal device for data-intensive tasks. We'll discuss GPU accelerator
            fundamentals as well as the best practices when developing your applications for modern GPU architectures.
            Both the architecture and the programming model evolved over the past few years to help developers achieve
            high performance quicker and with less effort. New features, such as Unified Memory, have been introduced to
            simplify development on heterogeneous architectures and provide seamless processing of large out-of-core
            data workloads. New libraries, such as nvGraph, make it possible to build interactive and high throughput
            graph and data analytics applications. An overview of existing tools and libraries will be covered to help
            you get started with the GPU programming.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Nikolay Sakharnykh (Nvidia)</summary>
            Nikolay Sakharnykh is a senior developer technology engineer at NVIDIA, where he works on accelerating HPC
            and data analytics applications on GPUs. He joined NVIDIA in 2008 as a graphics engineer working on making
            video games run faster and enabling new visual effects. At the same time, CUDA started to pick up, and he
            got excited about the general compute capabilities of the GPUs. After a few years his professional interests
            shifted towards more serious applications in HPC. Now he's exploring GPU applications for graph and data
            analytics and new memory management techniques.
          </details>
        </div>
      </div>
    </div>

    <!-- 2018 -->
    <div class="timeline right">
      <div class="card">
        <div class="card-body p-4">
          <h3>2018</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">How Persistent Memory Changes the Server Environment
            </summary>
            New memory technologies bring with them an explosion in memory capacities, offering multiple terabytes per
            CPU socket. But more than that – this new, large capacity memory is persistent! Andy will describe how this
            technology changes the server environment seen in data centers and clouds. He will explain the value of
            persistent memory, what it means to applications such as databases, and summarize what application vendors
            are doing to prepare for it. Andy will describe the work done by SNIA, the Storage Networking Industry
            Association, to align the industry on a unified programming model for persistent memory. He’ll show
            libraries and applications that have built on that model and describe the value they’ve demonstrated.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Andy Rudoff (Intel)</summary>
            Andy Rudoff is a Senior Principal Engineer at Intel Corporation, focusing on Non-Volatile Memory
            programming. He is a contributor to the SNIA NVM Programming Technical Work Group. His more than 30 years
            industry experience includes design and development work in operating systems, file systems, networking, and
            fault management at companies large and small, including Sun Microsystems and VMware. Andy has taught
            various Operating Systems classes over the years and is a co-author of the popular UNIX Network Programming
            text book.
          </details>
          <hr>
          <details>
            <summary style="list-style: none; color: #334d7d;">Active Heterogeneous Hardware and its Impact on System
              Design</summary>
            The rise of hardware heterogeneity and the potential to offload compute closer to data (e.g., storage and
            memory) or to push operations down to where data moves (e.g., on the networks or acceleration within the
            chip) opens both exciting opportunities and significant challenges for system software like databases that
            want to make efficient use of future hardware. One of the main questions is then, who absorbs that
            complexity especially as we move to the “noisy” cloud? In my talk, I will argue that addressing such a
            challenge requires an effort that is beyond what can be typically done within a single layer of the system
            stack. My proposal calls for a holistic approach by opening up the interfaces and customising the system
            stack for modern data processing workloads.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Jana Giceva (Imperial College)</summary>
            Jana Giceva is an assistant professor in the Department of Computing at Imperial College London, where she
            is part of the LSDS (Large Scale Data and Systems) group. Prior to that she completed her MSc and Phd in the
            Systems Group at ETH Zurich, where she was advised by Gustavo Alonso and co-advised by Timothy Roscoe. Her
            research interests are in systems support for Big Data and Data science to enable efficient use of modern
            and future hardware. The scope of her research spans multiple systems areas: from the data processing layer
            to operating systems, including hardware accelerators for data processing. She is the recipient of the ETH
            medal for her PhD dissertation awarded in 2017 and the Google European PhD Fellowship in operating systems
            in 2014.
          </details>
          <hr>
          <details>
            <summary style="list-style: none; color: #334d7d;">Scaling database systems to high-performance computers
            </summary>
            Analyzing massive datasets quickly requires scaling foundational data processing algorithms to the
            unprecedented compute, network and I/O concurrency of a modern datacenter. However, the software building
            blocks that are readily available today have largely been designed for high-performance computing
            applications and are profoundly unsatisfactory for I/O-intensive analytics. This talk highlights specific
            research challenges that need to be overcome to scale data processing to warehouse-scale computers, with
            particular focus on how to better utilize RDMA-capable networks, non-uniform network topologies, massively
            parallel file systems and NVMe-based storage in a disaggregated datacenter.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Spyros Blanas (Ohio State)</summary>
            Spyros Blanas is an assistant professor in the Department of Computer Science and Engineering at The Ohio
            State University. His research interest is high-performance database systems, and his current goal is to
            build a database system for high-end computing facilities. He has received the IEEE TCDE Rising Star Award
            and a Google Research Faculty award. He received his Ph.D. at the University of Wisconsin–Madison and part
            of his Ph.D. dissertation was commercialized in Microsoft’s flagship data management product, SQL Server, as
            the Hekaton in-memory transaction processing engine.
          </details>
          <hr>
          <details>
            <summary style="list-style: none; color: #334d7d;">Designing Data Management Systems in the Age of Dark
              Silicon</summary>
            Dennard scaling, which enables keeping the power density of the transistors constant, does not hold anymore.
            Even though we would be able to keep packing more cores in processors, we won’t be able to power all of them
            up simultaneously. This trend is referred to as dark silicon and fundamentally alters the focus of hardware
            design. In this new era, the focus needs to shift toward optimizing energy per instruction. This talk
            focuses on the implications of dark silicon and emerging hardware on the design of data management systems.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Pinar Tözün (IT University of Copenhagen)</summary>
            Pınar Tözün is an Associate Professor at IT University of Copenhagen. Before ITU, she was a research staff
            member at IBM Almaden Research Center. Prior to joining IBM, she received her PhD from EPFL. Her research
            focuses on HTAP engines, performance characterization of database workloads, and scalability and efficiency
            of data management systems on modern hardware. She received a Jim Gray Doctoral Dissertation Award Honorable
            Mention in 2016.
          </details>
        </div>
      </div>
    </div>

    <!-- 2019 -->
    <div class="timeline left">
      <div class="card">
        <div class="card-body p-4">
          <h3>2019</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">Performance Scaling with Innovative Compute Architectures
              and FPGAs</summary>
            Performance scaling and power efficiency with traditional computing architectures becomes increasingly
            challenging as next generation technology nodes provide diminishing performance and energy benefits. FPGAs
            with their reconfigurable circuits can tailor the hardware to the application through customized arithmetic
            and innovative compute and memory architectures, thereby exposing further potential for performance scaling.
            This has stimulated significant interest for their exploitation in compute intensive applications. During
            this talk, we discuss some examples of these innovative customized compute architectures in the context of
            data processing and show how these unleash new levels of performance scalability and compute efficiency.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Michaela Blott (Xilinx Research)</summary>
            Michaela Blott is a Distinguished Engineer at Xilinx Research, where she is heading a team of international
            scientists, driving research into new application domains for Xilinx devices, such as machine learning, in
            both embedded and hyperscale deployments. She graduated from the University of Kaiserslautern in Germany and
            brings over 25 years of experience in computer architecture, FPGA and board design, working in both research
            institutions (ETH Zurich and Bell Labs) as well as development organizations.
          </details>
          <hr>
          <details>
            <summary style="list-style: none; color: #334d7d;">Dark silicon — a currency we do not control</summary>
            The breakdown of dennard scaling changed the game of processor design: no longer can the entire die be
            filled with “always-on” components – some regions must be powered up and down at runtime to prevent the chip
            from overheating. Such “dim” or “dark” silicon is the new currency of chip design, raising the question:
            what functionality should be implemented in dark silicon? Viable candidates are any non-essential units that
            support important applications. Naturally, database researchers were quick to claim this resource, arguing
            that it should be used to implement instructions and primitives supporting database workloads. In this talk,
            we argue that, due to economic constraints, such a design is unlikely to be implemented in mainstream server
            chips. Instead, chip designers will spend silicon on high-volume market segments such as AI, Security or
            Graphics/AR which require a different set of primitives. Consequently, database researchers need to find
            uses for the actual functionality of chips rather than wishing for features that are economically
            infeasible. Let us develop innovative ways to exploit the “hardware we have, not the hardware we wish to
            have at a later time”. In the talk, we discuss examples of creative use of hardware for data management
            purposes such as TLBs for MVCC, Transactional Memory for statistics collection and hardware graphics shaders
            for data analytics. We also highlight some processor functionality that still calls for creative use such as
            many floating point instructions, integrated sound processors and some of the model-specific registers.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Holger Pirk (Imperial College)</summary>
            Holger Pirk is an assistant professor (“Lecturer” in traditional English terms) in the Department of
            Computing at Imperial College London. As such, he is a member of the Large-Scale Data and Systems Group. He
            is interested in all things data: analytics, transactions, systems, algorithms, data structures, processing
            models and everything in between. While most of his work targets “traditional” relational databases, his
            declared goal is to broaden the applicability of data management techniques. This means targeting new
            platforms like GPUs or FPGAs but also new applications like compilers, games and AI. Before joining
            Imperial, Holger was a Postdoc at the Database group at MIT CSAIL. He spent his PhD years in the Database
            Architectures group at CWI in Amsterdam resulting in a PhD from the University of Amsterdam in 2015.
          </details>
          <hr>
          <details>
            <summary style="list-style: none; color: #334d7d;">Building real database systems on real persistent memory</summary>
            “Real” persistent memory, such as Intel Optane DC PMM, offers high density, persistence and speed in between
            flash and DRAM. This changes the way we deal with storage devices in database systems - it is
            byte-addressable like memory, yet it is also persistent. Systems researchers have been keen in exploring its
            use since more than 10 years ago, to build persistent indexes, new file systems, persistent queues, faster
            logging and better replication approaches. Yet almost all previous work had to be done in simulated
            environments. Now it is time to look back, rethink, and devise practical, innovative ways of exploiting real
            persistent memory in database systems. In this talk, we discuss our recent experience with real Optane DC
            PMMs and the implications and future roles of persistent memory in database systems. In particular, we
            highlight the challenges and issues that were not well understood in simulated environments, such as
            programming model and resource contention between DRAM and persistent memory.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Tianzheng Wang (Simon Fraser University)</summary>
            Tianzheng Wang is an assistant professor in the School of Computing Science at Simon Fraser University in
            Canada (since Fall 2018). He works on the boundary between software and hardware to build better systems by
            fully utilizing the underlying hardware. His current research focuses on database systems and related
            systems areas that impact the design of database systems, such as operating systems, distributed systems,
            and synchronization. He is also interested in storage, mobile and embedded systems. Tianzheng Wang received
            his Ph.D. in computer science from the University of Toronto in 2017, advised by Ryan Johnson and Angela
            Demke Brown. Prior to joining Simon Fraser University, he spent one year (2017-2018) at Huawei Canada
            Research Centre (Toronto) as a research engineer.
          </details>
        </div>
      </div>
    </div>

    <div class="timeline right">
      <div class="card">
        <div class="card-body p-4">
          <h3>2020</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">A Vision for Expandable Data Management Infrastructure
              and Acceleration with Heterogeneous Configurable Systems</summary>
            Coherently attached FPGAs will unlock the full potential of their configurable fabric by enabling expandable
            memory and bringing together the four components of a data management system – storage, memory, compute, and
            network. In this configuration, the CPU can access the memory or storage attached to the FPGA coherently,
            while the reconfigurable fabric is able to support look-aside and inline acceleration for CPU/storage,
            CPU/memory, CPU/network, network/storage paths. Computational storage as well as computational memory will
            be facilitated by the same fabric that resides at the core of the heterogeneous compute systems. In this
            talk, we will present a vision of how heterogeneous compute systems centered around FPGAs can help with TCO,
            performance, and power density and the use cases that support such vision.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">José Roberto Alvarez (Intel Programmable Solution Group)
            </summary>
            José Roberto Alvarez is Senior Director at Intel Programmable Solutions Group in San Jose, California, where
            he leads the Technology and Innovation CTO Office, defining and implementing long-term FPGA research
            strategy and roadmaps. He started his career at Philips Laboratories and throughout his career he has been
            deeply engaged in architecting, designing and implementing technology products for a variety of industries
            including broadcast, embedded, consumer, post-production and computer graphics for companies including
            Philips, S3, Broadcom, Maxim, Xilinx, and four successful start-ups in Silicon Valley.
          </details>
        </div>
      </div>
    </div>
    <div class="timeline left">
      <div class="card">
        <div class="card-body p-4">
          <h3>2021</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">Introduction to the Arm Neoverse N and V series:
              Cloud-to-Edge Infrastructure SoCs</summary>
            In this talk I will present the Neoverse IP roadmap, detailing some of the characteristics of our IPs that
            make them a great choice for developing high-performance SoCs from power-constrained edge appliances all the
            way up to systems targeting HPC and cloud deployments. Additionally, this talk will touch upon the state of
            cloud and SW applications, and will provide some pointers to users that want to extract more performance and
            value from Arm Neoverse instances that are now easily accessible in various cloud environments.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Andrea Pellegrini (Arm)</summary>
            Andrea Pellegrini leads the performance and workloads team for the Infrastructure Line of Business at Arm.
            Andrea is based in Austin, TX, USA and joined Arm to work on Arm servers in 2016, after spending 3 years at
            Intel, where he was an architect for IO virtualization. Andrea obtained a PhD in computer architecture from
            the University of Michigan, Ann Arbor, and holds a Master and a Bachelors degree in computer engineering
            from the Universita' di Bologna, Italy.
          </details>
          <hr>
          <details>
            <summary style="list-style: none; color: #334d7d;">Extend, Not Just Accelerate!</summary>
            The hardware in today's datacenters and clouds is changing at a dizzying pace. Heterogeneity, accelerators
            and disaggregated architectures are becoming commonplace and change the way we design and operate databases.
            It has never been so easy to add an accelerator to a database but, in this talk, I will make the case that
            there is an alternative approach to be considered. Instead of building yet another analytics accelerator, we
            should use specialized hardware to offer new functionality in databases; functionality that makes them more
            secure, private and reliable! By using specialized hardware, the cost of such new functionality could be
            hidden, making future databases just as fast as today's while offering added benefits.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Zsolt István (IT University of Copenhagen)</summary>
            Zsolt István is an Associate Professor at the IT University of Copenhagen. Before that, he was an Assistant
            Research Professor at the IMDEA Software Institute in Madrid. Zsolt works in the intersection of databases,
            distributed systems, and FPGA programming. He has a PhD in Computer Science from the Systems Group at ETH
            Zurich, Switzerland.
          </details>
          <hr>
          <details>
            <summary style="list-style: none; color: #334d7d;">Cloud-native databases: opportunities and challenges</summary>
            Organizations are moving their databases to the cloud due to lower cost, elastic resource allocation,
            availability, etc. Cloud brings unique opportunities and challenges that are unprecedented in conventional
            databases, which require us to revisit the software and hardware stacks of a DBMS to fully exploit the
            performance and cost potential. This talk focuses on a particular architectural feature of cloud-native
            databases — storage disaggregation, where computation and storage are independently managed and connected
            through the network. Disaggregation enables independent scaling of resources, but incurs long latency and
            bandwidth bottlenecks for IO since the storage is remote. I will share our recent papers (choosing
            cloud-DBMS [VLDB'19], pushdownDB [ICDE'20], and FlexPushdownDB [VLDB'21]) addressing these challenges. I
            will also share my thoughts on the potential research questions and solutions in this domain.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Xiangyao Yu (University of Wisconsin-Madison)</summary>
            Xiangyao Yu is an Assistant Professor at the University of Wisconsin-Madison. His research interests include
            (1) transactions and HTAP, (2) new hardware for databases, and (3) cloud-native databases. Before joining
            UW-Madison, he finished his postdoc (2019) and PhD (2017) at MIT and his bachelor degree (2012) at Tsinghua
            University.
          </details>
        </div>
      </div>
    </div>
    <div class="timeline right">
      <div class="card">
        <div class="card-body p-4">
          <h3>2022</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">In Computer Architecture, We Don't Change the Questions,
              We Change the Answers</summary>
            When I was a new professor in the late 1980s, my senior colleague Jim Goodman told me, "On the computer
            architecture PhD qualifying exam, we don't change the questions, we only change the answers." More
            generally, I now augment this to say, "In computer architecture, we don't change the questions, application
            and technology innovations change the answers, and it's our job to recognize those changes." Eternal
            questions this talk will sample are how best to do the following interacting factors: compute, memory,
            storage, interconnect/networking, security, power, cooling and one more. The talk will not provide the
            answers but leave that as an audience exercise. I will dive a little more into compute and memory as
            in-progress trends provide both challenges and opportunities for creating tremendous value from (large)
            data.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Mark D. Hill (Microsoft Azure and University of
              Wisconsin-Madison)</summary>
            Mark D. Hill is Partner Hardware Architect with Microsoft Azure (2020-present) where he leads
            software-hardware pathfinding. He is also the Gene M. Amdahl and John P. Morgridge Professor Emeritus of
            Computer Sciences at the University of Wisconsin-Madison (https://www.cs.wisc.edu/~markhill/), following his
            1988-2020 service in Computer Sciences and Electrical and Computer Engineering.
          </details>
          <hr>
          <details>
            <summary style="list-style: none; color: #334d7d;">Accelerating Video Database Systems using Emerging
              Hardware Technologies</summary>
            Over the last decade, advances in deep learning have led to a resurgence of interest in automated analysis
            of videos at scale. This approach poses many challenges, ranging from the high computational overhead
            associated with deep learning models to the types of queries that the user may ask. In this talk, I will
            present EVA, an end-to-end video database system that we are developing at Georgia Tech, for tackling these
            challenges using novel query optimization and machine learning techniques. I will then discuss about
            opportunities for the community to help accelerate video database systems using their expertise in
            leveraging emerging hardware technologies.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Joy Arulraj (Georgia Institute of Technology)</summary>
            Joy Arulraj is an Assistant Professor of Computer Science at the Georgia Institute of Technology. His
            research focuses on developing systems for efficiently and effortlessly querying video datasets by
            synthesizing techniques from data systems and machine learning. His research has been recognized with the
            IEEE TCDE Early Career Award (2022) and the ACM SIGMOD Jim Gray Doctoral Dissertation Award (2019).
          </details>
          <hr>
          <details>
            <summary style="list-style: none; color: #334d7d;">What the Primacy of Economics Means for Hardware And Software</summary>
            Using several historical examples, I will argue that both hardware and software is downstream from
            economics. Economics has been called "the dismal science" and harsh economic realities can prevent
            technological breakthroughs. At the same time, however, economic thinking can also help overcome seemingly
            inescapable tradeoffs that we face when building software systems. It may also be our only hope for managing
            the proliferation of complex heterogeneous hardware, in particular in the cloud.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Viktor Leis (Friedrich-Alexander-Universität Erlangen-Nürnberg)</summary>
            Viktor Leis is a Professor for Data Management at Friedrich-Alexander University Erlangen-Nürnberg, Germany.
            His research revolves around designing high-performance data management systems and includes core database
            topics such as query processing, query optimization, index structures, and storage.
          </details>
        </div>
      </div>
    </div>
    <div class="timeline left">


      <div class="card">
        <div class="card-body p-4">
          <h3>2023</h3>
          <details>
            <summary style="list-style: none; color: #334d7d;">Memory: The DaMoN Demon</summary>
            Memory technology limitations bedevil current computing systems and data management does not escape. As
            silicon scaling delivers ever faster compute, memory falls further behind exposing capacity, bandwidth, and
            power deficiencies in our systems. Seeing these issues, computer architects propose memory hierarchy changes
            only to find most applications shrink-wrapped to the current hierarchy and unable to change. Data management
            applications provide an innovation bright-spot with researchers and developers ready to co-optimize from
            application-to-hardware to deliver improved performance. Hierarchy improvements often matter here first.
            Sitting squarely at this confluence, DaMoN serves to engender such co-optimizations. With this in mind, we
            will set a memory technology baseline using memory silicon trends and constraints. Additionally, we will set
            a memory system baseline summarizing current system memory architectures and issues. Next, we will look at
            the undeniable influence AI is exerting on the hierarchy. Taking memory technology, systems, and
            applications together, we will speculate on memory hierarchy changes to expect – potentially creating
            opportunities for future data management applications. One such change is already visible in CXL-enabled
            memory hierarchies envisioned to deliver higher capacity and perhaps more. Finally, we will speculate
            further on system optimizations around memory that are the subject of current research, like memory sharing
            and near memory computing. Active audience engagement is encouraged, as the goal of this presentation is a
            maximally productive DaMoN focused on vanquishing the memory demon.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Frank Hady (Intel Fellow, Intel’s Office of the CTO
              Systems Architecture and Engineering Group)</summary>
            Frank Hady is an Intel Fellow responsible for memory and storage hierarchy innovation within Intel’s Office
            of the CTO Systems Architecture and Engineering Group. He is a long-time system researcher happiest when
            delivering innovations that span hardware and software. Over Frank’s three-decade career, he has contributed
            to the creation, delivery, and proliferation of fundamental systems technologies.
          </details>
          <hr>
          <details>
            <summary style="list-style: none; color: #334d7d;">Cost-Intelligent Data Analytics in the Cloud</summary>
            For decades, database research has focused on optimizing performance under a fixed amount of resources. As
            more and more database applications move to the public cloud, we argue that it is time to make cost a
            first-class citizen when solving database optimization problems. In this talk, I will introduce the concept
            of “cost intelligence” and then sketch the architecture of a cloud data warehouse designed toward this goal.
            The project is in its early stages, and we would appreciate your valuable feedback.
          </details>
          <details>
            <summary style="list-style: none; color: #e1ba7b;">Huanchen Zhang, (Tsinghua University)</summary>
            Huanchen Zhang is an Assistant Professor in the IIIS (Yao Class) at Tsinghua University. His research
            interest is in database management systems with particular interests in indexing, data compression, and
            cloud databases.
          </details>
        </div>
      </div>
    </div>
  </div>
</div>